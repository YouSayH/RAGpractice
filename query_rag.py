import os
import torch
import yaml
import chromadb
from sentence_transformers import SentenceTransformer
from dotenv import load_dotenv

# --- LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã«å¿œã˜ã¦å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ ---
try:
    import google.generativeai as genai
except ImportError:
    genai = None

try:
    from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
except ImportError:
    AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig = None, None, None

try:
    import openai
except ImportError:
    openai = None


def load_config(config_path="config.yaml"):
    """YAMLè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€"""
    with open(config_path, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)

def transform_query(query_text: str, config: dict) -> str:
    """LLMã‚’ä½¿ã£ã¦ã€å…ƒã®è³ªå•ã‚’æ¤œç´¢ã«é©ã—ãŸå½¢ã«å¤‰æ›ã™ã‚‹ (Gemini APIã‚’ä½¿ç”¨)"""
    print("ğŸ”„ å…ƒã®è³ªå•ã‚’æ¤œç´¢ç”¨ã«å¤‰æ›ã—ã¦ã„ã¾ã™...")
    load_dotenv()
    GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")

    if not GOOGLE_API_KEY or not genai:
        print("âš ï¸ GOOGLE_API_KEYãŒè¦‹ã¤ã‹ã‚‰ãªã„ã‹ã€'google-generativeai'ãŒæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã®ãŸã‚ã€ã‚¯ã‚¨ãƒªå¤‰æ›ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return query_text

    try:
        genai.configure(api_key=GOOGLE_API_KEY)
        transform_llm = genai.GenerativeModel(config['generation']['gemini']['model_name'])
        
        prompt = f"""ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®ä»¥ä¸‹ã®è³ªå•ã«ç­”ãˆã‚‹ãŸã‚ã«æœ€ã‚‚é©ã—ã¦ã„ã‚‹ã§ã‚ã‚ã†ã€æ¶ç©ºã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆï¼ˆæ–‡ç« ï¼‰ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
ã“ã®ç”Ÿæˆã•ã‚ŒãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¯ã€ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã®ã‚¯ã‚¨ãƒªã¨ã—ã¦ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚å›ç­”ãã®ã‚‚ã®ã§ã¯ãªãã€æ¤œç´¢å¯¾è±¡ã¨ã—ã¦æœ€ã‚‚ç†æƒ³çš„ãªæ–‡ç« ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

# è³ªå•:
{query_text}

# ç”Ÿæˆã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ:
"""
        response = transform_llm.generate_content(prompt)
        transformed_query = response.text.strip()
        print(f"âœ… å¤‰æ›å¾Œã®ã‚¯ã‚¨ãƒª: ã€Œ{transformed_query[:80]}...ã€")
        return transformed_query
    except Exception as e:
        print(f"âš ï¸ ã‚¯ã‚¨ãƒªå¤‰æ›ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return query_text

def generate_with_gemini(prompt: str, config: dict) -> str:
    """Gemini APIã§å›ç­”ã‚’ç”Ÿæˆ"""
    if not genai:
        return "ã‚¨ãƒ©ãƒ¼: 'google-generativeai'ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚"
    api_config = config['generation']['gemini']
    print(f"\nğŸ¤– Gemini API ({api_config['model_name']}) ã§å›ç­”ã‚’ç”Ÿæˆä¸­...")
    try:
        llm = genai.GenerativeModel(api_config['model_name'])
        response = llm.generate_content(prompt)
        return response.text
    except Exception as e:
        return f"ã‚¨ãƒ©ãƒ¼: Gemini APIã§ã®å›ç­”ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚è©³ç´°: {e}"

def generate_with_huggingface(prompt: str, device: str, config: dict) -> str:
    """Hugging Faceã®ãƒ¢ãƒ‡ãƒ«ã‚’ç›´æ¥å®Ÿè¡Œã—ã¦å›ç­”ã‚’ç”Ÿæˆ"""
    if not AutoTokenizer:
        return "ã‚¨ãƒ©ãƒ¼: 'transformers'ã¾ãŸã¯'bitsandbytes'ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚"
    local_config = config['generation']['huggingface']
    print(f"\nğŸ¤– Hugging Faceãƒ¢ãƒ‡ãƒ« ({local_config['model_name']}) ã§å›ç­”ã‚’ç”Ÿæˆä¸­...")
    
    try:
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.float16,
        )
        tokenizer = AutoTokenizer.from_pretrained(local_config['model_name'])
        model = AutoModelForCausalLM.from_pretrained(
            local_config['model_name'],
            quantization_config=bnb_config,
            device_map="auto"
        )
        
        tokenized_prompt = tokenizer.encode(prompt, return_tensors="pt").to(device)
        generation_output = model.generate(tokenized_prompt, max_new_tokens=1024, temperature=0.7)
        prompt_length = len(tokenized_prompt[0])
        answer = tokenizer.decode(generation_output[0][prompt_length:], skip_special_tokens=True)
        return answer.strip()
    except Exception as e:
        return f"ã‚¨ãƒ©ãƒ¼: Hugging Faceãƒ¢ãƒ‡ãƒ«ã®å®Ÿè¡Œã«å¤±æ•—ã—ã¾ã—ãŸã€‚è©³ç´°: {e}"

def generate_with_lmstudio(prompt: str, config: dict) -> str:
    """LM StudioçµŒç”±ã§å›ç­”ã‚’ç”Ÿæˆ"""
    if not openai:
        return "ã‚¨ãƒ©ãƒ¼: 'openai'ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚"
    local_config = config['generation']['lmstudio']
    print(f"\nğŸ¤– LM Studio ({local_config['base_url']}) çµŒç”±ã§å›ç­”ã‚’ç”Ÿæˆä¸­...")
    
    try:
        client = openai.OpenAI(base_url=local_config['base_url'], api_key=local_config['api_key'])
        completion = client.chat.completions.create(
            model=local_config['model'],
            messages=[
                {"role": "system", "content": "ã‚ãªãŸã¯å„ªç§€ãªAIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚æä¾›ã•ã‚ŒãŸå‚è€ƒæƒ…å ±ã«åŸºã¥ã„ã¦ã€æ—¥æœ¬èªã§è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.7,
        )
        return completion.choices[0].message.content.strip()
    except Exception as e:
        return f"âŒ LM Studioã¨ã®é€šä¿¡ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}\nLM Studioã§ã‚µãƒ¼ãƒãƒ¼ãŒèµ·å‹•ã—ã¦ã„ã‚‹ã‹ã€è¨­å®šãŒæ­£ã—ã„ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚"


def query_rag(query_text: str, config: dict) -> str:
    """RAGã®è³ªç–‘å¿œç­”ã‚’å®Ÿè¡Œã™ã‚‹ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    db_config = config['database']
    embedding_config = config['embedding']
    gen_config = config['generation']
    provider = gen_config.get('provider', 'gemini') # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯gemini

    # --- 1. æº–å‚™ (å…±é€š) ---
    if not os.path.exists(db_config['persist_directory']):
        return f"ã‚¨ãƒ©ãƒ¼: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ '{db_config['persist_directory']}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    embedding_model = SentenceTransformer(embedding_config['model_name'], device=device)
    client = chromadb.PersistentClient(path=db_config['persist_directory'])
    collection = client.get_collection(name=db_config['collection_name'])

    # --- 2. ã‚¯ã‚¨ãƒªå¤‰æ›ã¨æ¤œç´¢ (Retrieval) ---
    transformed_query = transform_query(query_text, config)
    
    print("\n-------------------------------------------")
    print(f"å…ƒã®è³ªå•: {query_text}")
    print(f"å¤‰æ›å¾Œã®æ¤œç´¢ã‚¯ã‚¨ãƒª: {transformed_query}")
    print("-------------------------------------------")

    search_query = transformed_query
    if transformed_query != query_text:
        while True:
            choice = input("ã©ã¡ã‚‰ã®ã‚¯ã‚¨ãƒªã§æ¤œç´¢ã—ã¾ã™ã‹ï¼Ÿ [1: å¤‰æ›å¾Œ, 2: è‡ªåˆ†ã§å…¥åŠ›, Enter: å¤‰æ›å¾Œ]: ")
            if choice == '1' or choice == '':
                break
            elif choice == '2':
                search_query = input("æ–°ã—ã„æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: ")
                break
            else:
                print("1ã‹2ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")

    print(f"\nğŸ” æœ€çµ‚ã‚¯ã‚¨ãƒªã€Œ{search_query[:80]}...ã€ã§æƒ…å ±ã‚’æ¤œç´¢ä¸­...")
    query_embedding = embedding_model.encode(search_query, normalize_embeddings=True).tolist()
    
    results = collection.query(
        query_embeddings=[query_embedding],
        n_results=3 
    )
    
    if not results['documents'][0]:
        return "é–¢é€£ã™ã‚‹æƒ…å ±ãŒãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
    
    context_for_prompt = "\n\n---\n\n".join(results['documents'][0])
    print(f"âœ… é–¢é€£æƒ…å ±ã‚’{len(results['documents'][0])}ä»¶ç™ºè¦‹ã€‚LLMã«æ¸¡ã—ã¾ã™ã€‚")

    # --- 3. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆã¨ç”Ÿæˆ ---
    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
    base_prompt = f"""ä»¥ä¸‹ã®ã€Œå‚è€ƒæƒ…å ±ã€ã ã‘ã‚’å³å¯†ã«å‚ç…§ã—ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã€Œè³ªå•ã€ã«ç­”ãˆã¦ãã ã•ã„ã€‚
å‚è€ƒæƒ…å ±ã«ç­”ãˆã‚„é–¢é€£æƒ…å ±ãŒãªã„å ´åˆã¯ã€æ†¶æ¸¬ã§ç­”ãˆãšã€æ­£ç›´ã«ã€Œåˆ†ã‹ã‚Šã¾ã›ã‚“ã€ã¨å›ç­”ã—ã¦ãã ã•ã„ã€‚

# å‚è€ƒæƒ…å ±
{context_for_prompt}

# è³ªå•
{query_text}
"""
    
    # ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã«å¿œã˜ã¦å‡¦ç†ã‚’åˆ†å²
    if provider == 'gemini':
        return generate_with_gemini(base_prompt, config)
        
    elif provider == 'huggingface':
        # Hugging Face (Gemma) ç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå½¢å¼ã«å¤‰æ›
        hf_prompt = f"<start_of_turn>user\n{base_prompt}<end_of_turn>\n<start_of_turn>model\n"
        return generate_with_huggingface(hf_prompt, device, config)
        
    elif provider == 'lmstudio':
        # LM Studio (Chatå½¢å¼) ã§ã¯ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ãã®ã¾ã¾æ¸¡ã™
        return generate_with_lmstudio(base_prompt, config)
        
    else:
        return f"ã‚¨ãƒ©ãƒ¼: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã«ã‚ã‚‹ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ '{provider}' ã¯ç„¡åŠ¹ã§ã™ã€‚"


if __name__ == "__main__":
    config = load_config()

    while True:
        question = input("\nè³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ (çµ‚äº†ã™ã‚‹ã«ã¯ 'exit' ã¨å…¥åŠ›): ")
        if question.lower() == 'exit':
            print("çµ‚äº†ã—ã¾ã™ã€‚")
            break
        
        final_answer = query_rag(question, config)
        print("\nğŸ’¡ æœ€çµ‚çš„ãªå›ç­”:")
        print("===============================")
        print(final_answer)