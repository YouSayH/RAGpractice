import os
import torch
import yaml
import chromadb
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForCausalLM
import google.generativeai as genai
from dotenv import load_dotenv

def load_config(config_path="config.yaml"):
    """YAMLè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€"""
    with open(config_path, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)

def transform_query(query_text: str, llm) -> str:
    """
    LLMã‚’ä½¿ã£ã¦ã€å…ƒã®è³ªå•ã‚’æ¤œç´¢ã«é©ã—ãŸå½¢ã«å¤‰æ›ã™ã‚‹ã€‚
    """
    print("ğŸ”„ å…ƒã®è³ªå•ã‚’æ¤œç´¢ç”¨ã«å¤‰æ›ã—ã¦ã„ã¾ã™...")
    
    prompt = f"""ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®ä»¥ä¸‹ã®è³ªå•ã«ç­”ãˆã‚‹ãŸã‚ã«æœ€ã‚‚é©ã—ã¦ã„ã‚‹ã§ã‚ã‚ã†ã€æ¶ç©ºã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆï¼ˆæ–‡ç« ï¼‰ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
ã“ã®ç”Ÿæˆã•ã‚ŒãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¯ã€ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã®ã‚¯ã‚¨ãƒªã¨ã—ã¦ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚å›ç­”ãã®ã‚‚ã®ã§ã¯ãªãã€æ¤œç´¢å¯¾è±¡ã¨ã—ã¦æœ€ã‚‚ç†æƒ³çš„ãªæ–‡ç« ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

# è³ªå•:
{query_text}

# ç”Ÿæˆã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ:
"""
    
    try:
        response = llm.generate_content(prompt)
        transformed_query = response.text.strip()
        print(f"âœ… å¤‰æ›å¾Œã®ã‚¯ã‚¨ãƒª: ã€Œ{transformed_query[:80]}...ã€")
        return transformed_query
    except Exception as e:
        print(f"âš ï¸ ã‚¯ã‚¨ãƒªå¤‰æ›ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return query_text

def query_rag(query_text: str, config: dict) -> str:
    """RAGã®è³ªç–‘å¿œç­”ã‚’å®Ÿè¡Œã™ã‚‹ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    db_config = config['database']
    embedding_config = config['embedding']
    gen_config = config['generation']

    # --- 1. æº–å‚™ (å…±é€š) ---
    if not os.path.exists(db_config['persist_directory']):
        return f"ã‚¨ãƒ©ãƒ¼: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ '{db_config['persist_directory']}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    embedding_model = SentenceTransformer(embedding_config['model_name'], device=device)
    client = chromadb.PersistentClient(path=db_config['persist_directory'])
    collection = client.get_collection(name=db_config['collection_name'])

    # --- 2. ã‚¯ã‚¨ãƒªå¤‰æ›ã¨æ¤œç´¢ (Retrieval) ---
    load_dotenv()
    GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
    if not GOOGLE_API_KEY:
        return "ã‚¨ãƒ©ãƒ¼: ã‚¯ã‚¨ãƒªå¤‰æ›ã®ãŸã‚ã«GOOGLE_API_KEYãŒå¿…è¦ã§ã™ã€‚"
    genai.configure(api_key=GOOGLE_API_KEY)
    transform_llm = genai.GenerativeModel(gen_config['api']['model_name'])
    
    transformed_query = transform_query(query_text, transform_llm)
    
    print("\n-------------------------------------------")
    print(f"å…ƒã®è³ªå•: {query_text}")
    print(f"å¤‰æ›å¾Œã®æ¤œç´¢ã‚¯ã‚¨ãƒª: {transformed_query}")
    print("-------------------------------------------")

    search_query = ""
    while True:
        choice = input("ã©ã¡ã‚‰ã®ã‚¯ã‚¨ãƒªã§æ¤œç´¢ã—ã¾ã™ã‹ï¼Ÿ [1: å¤‰æ›å¾Œ, 2: è‡ªåˆ†ã§å…¥åŠ›]: ")
        if choice == '1':
            search_query = transformed_query
            break
        elif choice == '2':
            search_query = input("æ–°ã—ã„æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: ")
            break
        else:
            print("1ã‹2ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")

    print(f"\nğŸ” æœ€çµ‚ã‚¯ã‚¨ãƒªã€Œ{search_query[:80]}...ã€ã§æƒ…å ±ã‚’æ¤œç´¢ä¸­...")
    query_embedding = embedding_model.encode(search_query, normalize_embeddings=True).tolist()
    
    results = collection.query(
        query_embeddings=[query_embedding],
        n_results=3 
    )
    
    if not results['documents'][0]:
        return "é–¢é€£ã™ã‚‹æƒ…å ±ãŒãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
    
    retrieved_documents = results['documents'][0]
    context_for_prompt = "\n\n---\n\n".join(retrieved_documents)
    
    print(f"âœ… é–¢é€£æƒ…å ±ã‚’{len(retrieved_documents)}ä»¶ç™ºè¦‹ã€‚LLMã«æ¸¡ã—ã¾ã™ã€‚")

    # --- 3.ç”Ÿæˆ(configã®å€¤ã«å¿œã˜ã¦åˆ‡ã‚Šæ›¿ãˆ) ---
    if gen_config['use_local_llm']:
        local_config = gen_config['local']
        print(f"\nğŸ¤– ãƒ­ãƒ¼ã‚«ãƒ«LLM ({local_config['model_name']}) ã§å›ç­”ã‚’ç”Ÿæˆä¸­...")
        
        tokenizer = AutoTokenizer.from_pretrained(local_config['model_name'])
        model = AutoModelForCausalLM.from_pretrained(
            local_config['model_name'], torch_dtype=torch.float16, load_in_4bit=True, device_map="auto"
        )
        prompt = f"""<start_of_turn>user
ä»¥ä¸‹ã®ã€Œå‚è€ƒæƒ…å ±ã€ã®ä¸­ã‹ã‚‰ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã€Œè³ªå•ã€ã«ç­”ãˆã‚‹ãŸã‚ã«å¿…è¦ãªæƒ…å ±ã ã‘ã‚’æŠœãå‡ºã—ã¦ã€å›ç­”ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

# å‚è€ƒæƒ…å ±
{context_for_prompt}

# è³ªå•
{query_text}<end_of_turn>
<start_of_turn>model
"""
        tokenized_prompt = tokenizer.encode(prompt, return_tensors="pt").to(device)
        generation_output = model.generate(tokenized_prompt, max_new_tokens=1024)
        prompt_length = len(tokenized_prompt[0])
        answer = tokenizer.decode(generation_output[0][prompt_length:], skip_special_tokens=True)
        return answer.strip()
    else:
        api_config = gen_config['api']
        print(f"\nğŸ¤– Gemini API ({api_config['model_name']}) ã§æœ€çµ‚å›ç­”ã‚’ç”Ÿæˆä¸­...")
        
        llm = genai.GenerativeModel(api_config['model_name'])
        
        prompt = f"""ä»¥ä¸‹ã®è¤‡æ•°ã®ã€Œå‚è€ƒæƒ…å ±ã€ã®ä¸­ã‹ã‚‰ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®ã€Œè³ªå•ã€ã«ç­”ãˆã‚‹ãŸã‚ã«å¿…è¦ãªæƒ…å ±ã ã‘ã‚’æŠ½å‡ºã—ã€ãã‚Œã«åŸºã¥ã„ã¦å›ç­”ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
å›ç­”ä»¥å¤–ã®ä½™è¨ˆãªè¨€è‘‰ï¼ˆã€Œå›ç­”ï¼šã€ãªã©ã®æ¥é ­è¾ã‚„ã€è³ªå•ã®ç¹°ã‚Šè¿”ã—ãªã©ï¼‰ã¯ä¸€åˆ‡å«ã‚ãªã„ã§ãã ã•ã„ã€‚

# å‚è€ƒæƒ…å ±
{context_for_prompt}

# è³ªå•
{query_text}
"""
        try:
            response = llm.generate_content(prompt)
            return response.text
        except Exception as e:
            return f"ã‚¨ãƒ©ãƒ¼: Gemini APIã§ã®å›ç­”ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚è©³ç´°: {e}"

if __name__ == "__main__":
    config = load_config()

    while True:
        question = input("\nè³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ (çµ‚äº†ã™ã‚‹ã«ã¯ 'exit' ã¨å…¥åŠ›): ")
        if question.lower() == 'exit':
            print("çµ‚äº†ã—ã¾ã™ã€‚")
            break
        
        final_answer = query_rag(question, config)
        print("\nğŸ’¡ æœ€çµ‚çš„ãªå›ç­”:")
        print("===============================")
        print(final_answer)