import os
import torch
import yaml
import chromadb
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForCausalLM
import google.generativeai as genai
from dotenv import load_dotenv

def load_config(config_path="config.yaml"):
    """YAMLè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€"""
    with open(config_path, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)

def query_rag(query_text: str, config: dict) -> str:
    """RAGã®è³ªç–‘å¿œç­”ã‚’å®Ÿè¡Œã™ã‚‹ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    db_config = config['database']
    embedding_config = config['embedding']
    gen_config = config['generation']

    # 1.æº–å‚™(å…±é€š)
    if not os.path.exists(db_config['persist_directory']):
        return f"ã‚¨ãƒ©ãƒ¼: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ '{db_config['persist_directory']}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    embedding_model = SentenceTransformer(embedding_config['model_name'], device=device)
    client = chromadb.PersistentClient(path=db_config['persist_directory'])
    collection = client.get_collection(name=db_config['collection_name'])

    # 2.æ¤œç´¢ (å…±é€š)
    print(f"ğŸ” è³ªå•ã€Œ{query_text}ã€ã«åŸºã¥ã„ã¦æƒ…å ±ã‚’æ¤œç´¢ä¸­...")
    query_embedding = embedding_model.encode(query_text, normalize_embeddings=True).tolist()
    results = collection.query(query_embeddings=[query_embedding], n_results=1)
    
    if not results['documents'][0]:
        return "é–¢é€£ã™ã‚‹æƒ…å ±ãŒãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
    
    retrieved_document = results['documents'][0][0]
    print(f"âœ… é–¢é€£æƒ…å ±ã‚’ç™ºè¦‹: ã€Œ{retrieved_document[:80]}...ã€")

    # 3.ç”Ÿæˆ(configã®å€¤ã«å¿œã˜ã¦åˆ‡ã‚Šæ›¿ãˆ)
    if gen_config['use_local_llm']:
        local_config = gen_config['local']
        print(f"\nãƒ­ãƒ¼ã‚«ãƒ«LLM ({local_config['model_name']}) ã§å›ç­”ã‚’ç”Ÿæˆä¸­...")
        
        tokenizer = AutoTokenizer.from_pretrained(local_config['model_name'])
        model = AutoModelForCausalLM.from_pretrained(
            local_config['model_name'], torch_dtype=torch.float16, load_in_4bit=True, device_map="auto"
        )
        prompt = f"""<start_of_turn>user
ä»¥ä¸‹ã®å‚è€ƒæƒ…å ±ã®ã¿ã‚’ä½¿ç”¨ã—ã¦ã€è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚

# å‚è€ƒæƒ…å ±
{retrieved_document}

# è³ªå•
{query_text}<end_of_turn>
<start_of_turn>model
"""
        tokenized_prompt = tokenizer.encode(prompt, return_tensors="pt").to(device)
        generation_output = model.generate(tokenized_prompt, max_new_tokens=150)
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ã—ã¦å…¥åŠ›ã—ãŸéƒ¨åˆ†ã®é•·ã•ã‚’å–å¾—
        prompt_length = len(tokenized_prompt[0])
        # ç”Ÿæˆã•ã‚ŒãŸéƒ¨åˆ†ã ã‘ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰
        answer = tokenizer.decode(generation_output[0][prompt_length:], skip_special_tokens=True)
        return answer.strip()
    else:# USE_LOCAL_LLM false
        api_config = gen_config['api']
        print(f"\nGemini API ({api_config['model_name']}) ã§æœ€çµ‚å›ç­”ã‚’ç”Ÿæˆä¸­...")
        
        load_dotenv()
        GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
        if not GOOGLE_API_KEY:
            return "ã‚¨ãƒ©ãƒ¼: GOOGLE_API_KEYãŒ.envãƒ•ã‚¡ã‚¤ãƒ«ã«è¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚"
            
        genai.configure(api_key=GOOGLE_API_KEY)
        llm = genai.GenerativeModel(api_config['model_name'])
        
        prompt = f"""ä»¥ä¸‹ã®ã€Œå‚è€ƒæƒ…å ±ã€ã‚’èª­ã¿ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®ã€Œè³ªå•ã€ã«ç­”ãˆã‚‹æ–‡ç« ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
å›ç­”ä»¥å¤–ã®ä½™è¨ˆãªè¨€è‘‰ï¼ˆã€Œå›ç­”ï¼šã€ãªã©ã®æ¥é ­è¾ã‚„ã€è³ªå•ã®ç¹°ã‚Šè¿”ã—ãªã©ï¼‰ã¯ä¸€åˆ‡å«ã‚ãªã„ã§ãã ã•ã„ã€‚

# å‚è€ƒæƒ…å ±
{retrieved_document}

# è³ªå•
{query_text}
"""
        try:
            response = llm.generate_content(prompt)
            return response.text
        except Exception as e:
            return f"ã‚¨ãƒ©ãƒ¼: Gemini APIã§ã®å›ç­”ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚è©³ç´°: {e}"

if __name__ == "__main__":
    config = load_config()

    # é€£ç¶šã—ã¦è³ªå•ã§ãã‚‹ãƒãƒ£ãƒƒãƒˆãƒ«ãƒ¼ãƒ—
    while True:
        question = input("\nè³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ (çµ‚äº†ã™ã‚‹ã«ã¯ 'exit' ã¨å…¥åŠ›): ")
        if question.lower() == 'exit':
            print("çµ‚äº†ã—ã¾ã™ã€‚")
            break
        
        final_answer = query_rag(question, config)
        print("\nğŸ’¡ æœ€çµ‚çš„ãªå›ç­”:")
        print("===============================")
        print(final_answer)