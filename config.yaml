# --- データベース設定 ---
database:
  persist_directory: "chroma_db"
  collection_name: "my_collection"
  source_directory: "source_documents"

# --- ベクトル化（Embedding）設定 ---
embedding:
  model_name: 'intfloat/multilingual-e5-large'

# --- チャンク化（テキスト分割）設定 ---
# build_database.py で使用
chunking:
  chunk_size: 1000
  chunk_overlap: 200

# --- 回答生成（Generation）設定 ---
generation:
  # ここで 'gemini', 'huggingface', 'lmstudio' のいずれかを選択
  provider: 'gemini' 

  # --- 各プロバイダーの詳細設定 ---
  # ① Gemini APIを使用する場合
  gemini:
    model_name: 'gemini-2.5-flash'

  # ② Hugging Faceのモデルを直接実行する場合
  huggingface:
    model_name: "google/gemma-1.1-7b-it" # Hugging Face上のモデルID  google/gemma-7b-it
  
  # ③ LM Studioのローカルサーバー経由で使用する場合
  lmstudio:
    base_url: "http://localhost:1234/v1" # LM Studioのサーバーアドレス
    model: "local-model" # LM Studio側でロードしているモデル（固定値でOK）
    api_key: "not-needed" # APIキー（不要）