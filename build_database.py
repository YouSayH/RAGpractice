import os
import glob
import torch
import yaml
from tqdm import tqdm
import chromadb
from sentence_transformers import SentenceTransformer
from langchain_community.document_loaders import (
    TextLoader,
    PyPDFLoader,
    Docx2txtLoader,
    UnstructuredExcelLoader,
    UnstructuredPowerPointLoader,
)
from langchain.text_splitter import MarkdownHeaderTextSplitter

def load_config(config_path="config.yaml"):
    """YAMLè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€"""
    with open(config_path, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)

def load_documents(directory_path):
    """æŒ‡å®šã•ã‚ŒãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰æ§˜ã€…ãªå½¢å¼ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã‚€"""
    loader_map = {
        ".txt": TextLoader,
        ".md": TextLoader,
        ".py": TextLoader,
        ".pdf": PyPDFLoader,
        ".docx": Docx2txtLoader,
        ".xlsx": UnstructuredExcelLoader,
        ".pptx": UnstructuredPowerPointLoader,
    }
    
    documents = []
    print(f"'{directory_path}' ã‹ã‚‰ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã‚“ã§ã„ã¾ã™...")
    
    all_files = glob.glob(os.path.join(directory_path, "**/*"), recursive=True)
    supported_files = [f for f in all_files if os.path.splitext(f)[1].lower() in loader_map]
    
    for file_path in supported_files:
        ext = os.path.splitext(file_path)[1].lower()
        loader_class = loader_map[ext]
        
        print(f"ğŸ“„ '{file_path}' ã‚’èª­ã¿è¾¼ã¿ä¸­...")
        try:
            if ext in [".txt", ".md", ".py"]:
                loader = loader_class(file_path, encoding="utf-8")
            else:
                loader = loader_class(file_path)
            
            documents.extend(loader.load())
        except Exception as e:
            print(f"Error loading {file_path}: {e}")

    return documents

def main():
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ§‹ç¯‰ã®ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    config = load_config()
    db_config = config['database']
    embedding_config = config['embedding']
    
    print("--- ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ§‹ç¯‰é–‹å§‹ ---")

    if not os.path.exists(db_config['source_directory']):
        print(f"ã‚¨ãƒ©ãƒ¼: ã‚½ãƒ¼ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª '{db_config['source_directory']}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return

    documents = load_documents(db_config['source_directory'])
    if not documents:
        print("èª­ã¿è¾¼ã‚€ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return
    print(f"\nåˆè¨ˆ {len(documents)} å€‹ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆï¼ˆãƒšãƒ¼ã‚¸ï¼‰ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚")

    # Markdownã®è¦‹å‡ºã—ãƒ¬ãƒ™ãƒ«2, 3, 4ã‚’åŸºæº–ã«ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’åˆ†å‰²ã™ã‚‹
    headers_to_split_on = [
        ("##", "Header 2"),
        ("###", "Header 3"),
        ("####", "Header 4"),
    ]

    markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)
    
    chunks = []
    for doc in documents:
        # å„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’åˆ†å‰²
        chunks.extend(markdown_splitter.split_text(doc.page_content))
    
    print(f"ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ {len(chunks)} å€‹ã®ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ã—ã¾ã—ãŸã€‚")

    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"\nãƒ‡ãƒã‚¤ã‚¹ '{device}' ã‚’ä½¿ç”¨ã—ã¦Embeddingãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚")
    embedding_model = SentenceTransformer(embedding_config['model_name'], device=device)
    
    client = chromadb.PersistentClient(path=db_config['persist_directory'])
    
    try:
        client.delete_collection(name=db_config['collection_name'])
        print(f"æ—¢å­˜ã®ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ '{db_config['collection_name']}' ã‚’å‰Šé™¤ã—ã¾ã—ãŸã€‚")
    except Exception:
        pass
    
    collection = client.create_collection(name=db_config['collection_name'])

    print("ãƒãƒ£ãƒ³ã‚¯ã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã—ã€ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜ã—ã¦ã„ã¾ã™...")
    for i, chunk in enumerate(tqdm(chunks, desc="Embedding Chunks")):
        text_content = chunk.page_content
        embedding = embedding_model.encode(text_content, normalize_embeddings=True).tolist()
        collection.add(
            ids=[f"chunk_{i}"],
            embeddings=[embedding],
            documents=[text_content]
        )

    print("\n--- ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ§‹ç¯‰å®Œäº† ---")
    print(f"åˆè¨ˆ {collection.count()} å€‹ã®ãƒãƒ£ãƒ³ã‚¯ãŒãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸã€‚")

if __name__ == "__main__":
    main()