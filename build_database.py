import os
import glob
import torch
import yaml
from tqdm import tqdm
import chromadb
from sentence_transformers import SentenceTransformer
from langchain_community.document_loaders import (
    TextLoader,
    PyPDFLoader,
    Docx2txtLoader,
    UnstructuredExcelLoader,
    UnstructuredPowerPointLoader,
)
from langchain.text_splitter import RecursiveCharacterTextSplitter

def load_config(config_path="config.yaml"):
    """YAMLè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€"""
    with open(config_path, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)

def load_documents(directory_path):
    """æŒ‡å®šã•ã‚ŒãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰æ§˜ã€…ãªå½¢å¼ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã‚€"""
    loader_map = {
        ".txt": TextLoader,
        ".md": TextLoader,
        ".py": TextLoader,
        ".pdf": PyPDFLoader,
        ".docx": Docx2txtLoader,
        ".xlsx": UnstructuredExcelLoader,
        ".pptx": UnstructuredPowerPointLoader,
    }
    
    documents = []
    print(f"'{directory_path}' ã‹ã‚‰ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã‚“ã§ã„ã¾ã™...")
    
    all_files = glob.glob(os.path.join(directory_path, "**/*"), recursive=True)
    supported_files = [f for f in all_files if os.path.splitext(f)[1].lower() in loader_map]
    
    for file_path in supported_files:
        ext = os.path.splitext(file_path)[1].lower()
        loader_class = loader_map[ext]
        
        print(f"ğŸ“„ '{file_path}' ã‚’èª­ã¿è¾¼ã¿ä¸­...")
        try:
            if ext in [".txt", ".md", ".py"]:
                loader = loader_class(file_path, encoding="utf-8")
            else:
                loader = loader_class(file_path)
            
            documents.extend(loader.load())
        except Exception as e:
            print(f"Error loading {file_path}: {e}")

    return documents

def main():
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ§‹ç¯‰ã®ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    config = load_config()
    db_config = config['database']
    embedding_config = config['embedding']
    chunking_config = config['chunking']
    
    print("--- ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ§‹ç¯‰é–‹å§‹ ---")

    if not os.path.exists(db_config['source_directory']):
        print(f"ã‚¨ãƒ©ãƒ¼: ã‚½ãƒ¼ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª '{db_config['source_directory']}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return

    documents = load_documents(db_config['source_directory'])
    if not documents:
        print("èª­ã¿è¾¼ã‚€ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return
    print(f"\nåˆè¨ˆ {len(documents)} å€‹ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆï¼ˆãƒšãƒ¼ã‚¸ï¼‰ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚")

    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunking_config['chunk_size'],
        chunk_overlap=chunking_config['chunk_overlap']
    )
    chunks = text_splitter.split_documents(documents)
    print(f"ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ {len(chunks)} å€‹ã®ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ã—ã¾ã—ãŸã€‚")

    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"\nãƒ‡ãƒã‚¤ã‚¹ '{device}' ã‚’ä½¿ç”¨ã—ã¦Embeddingãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚")
    embedding_model = SentenceTransformer(embedding_config['model_name'], device=device)
    
    client = chromadb.PersistentClient(path=db_config['persist_directory'])

    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ã‚¯ãƒªãƒ¼ãƒ³ãªå†æ§‹ç¯‰
    # ä»¥ä¸‹ã®å‡¦ç†ã«ã‚ˆã‚Šã€ã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Ÿè¡Œæ™‚ã«å¸¸ã«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ã¾ã£ã•ã‚‰ãªçŠ¶æ…‹ã‹ã‚‰ä½œã‚Šç›´ã—ã¾ã™ã€‚
    # ã“ã‚Œã«ã‚ˆã‚Šã€å…ƒãƒ•ã‚¡ã‚¤ãƒ«ãŒå‰Šé™¤ã•ã‚ŒãŸå ´åˆã«ã€å¤ã„æƒ…å ±ãŒDBã«æ®‹ã‚Šç¶šã‘ã‚‹ã®ã‚’é˜²ãã¾ã™ã€‚
    try:
        client.delete_collection(name=db_config['collection_name'])
        print(f"æ—¢å­˜ã®ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ '{db_config['collection_name']}' ã‚’å‰Šé™¤ã—ã¾ã—ãŸã€‚")
    except Exception:# ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½•ã‚‚ã—ãªã„
        pass
    
    collection = client.create_collection(name=db_config['collection_name'])

    print("ãƒãƒ£ãƒ³ã‚¯ã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã—ã€ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜ã—ã¦ã„ã¾ã™...")
    for i, chunk in enumerate(tqdm(chunks, desc="Embedding Chunks")):
        text = chunk.page_content
        embedding = embedding_model.encode(text, normalize_embeddings=True).tolist()
        collection.add(ids=[f"chunk_{i}"], embeddings=[embedding], documents=[text])

    print("\n--- ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ§‹ç¯‰å®Œäº† ---")
    print(f"åˆè¨ˆ {collection.count()} å€‹ã®ãƒãƒ£ãƒ³ã‚¯ãŒãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸã€‚")

if __name__ == "__main__":
    main()